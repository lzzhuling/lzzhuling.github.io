# jemdoc: notime

#= Welcome to my home page!

#~~~
#{}{img_left}{photos/pic1.jpg}{alt text}{256px}{301px}
= Linglingzhi Zhu

=== Ph.D. Student @ The Chinese University of Hong Kong

#=== Department of Systems Engineering and Engineering Management

#=== The Chinese University of Hong Kong


Email: llzzhu at se.cuhk.edu.hk or lzzhuling at gmail.com

[https://scholar.google.com.hk/citations?user=nOSAyisAAAAJ \[Google Scholar\]]

== About Me

I am currently a fourth-year Ph.D. student under the supervision of Professor [https://www1.se.cuhk.edu.hk/~manchoso/ Anthony Man-Cho So] at CUHK. 
I received a B.S. in mathematics in 2017 from Zhejiang University and M.S. in computational mathematics in 2020 under the supervision of Professor [https://scholar.google.com/citations?user=wXOSCJ0AAAAJ Chong Li] from Zhejiang University.
I was a research assistant under the mentorship of Professor [https://www.polyu.edu.hk/ama/profile/xqyang/ Xiaoqi Yang] during February 2019 to May 2019 at The Hong Kong Polytechnic University.  
#~~~

== Research Interests
Perturbation Theory, Geometric Optimization, Nonsmooth Analysis, Inverse Problems, Statistical Inference

== Publications 
. Differentially Private Online Federated Learning with Correlated Noise [https://arxiv.org/abs/2403.16542 \[arXiv\]]\n
Jiaojiao Zhang, *Linglingzhi Zhu*, Mikael Johansson. \n
preprint, 2024.
. Rotation Group Synchronization via Quotient Manifold [https://arxiv.org/abs/2306.12730 \[arXiv\]][https://lzzhuling.github.io/files/slides_sync_quotient.pdf \[slides\]]\n
*Linglingzhi Zhu*, Chong Li, Anthony Man-Cho So. \n
preprint, 2023.
. Universal Gradient Descent Ascent Method for Nonconvex-Nonconcave Minimax Optimization [https://lzzhuling.github.io/files/UGDA-NeurIPS23.pdf \[pdf\]]\n
Taoli Zheng, *Linglingzhi Zhu*, Anthony Man-Cho So, José Blanchet, Jiajin Li. \n
Advances in Neural Information Processing Systems 36 (NeurIPS 2023).
. Nonsmooth Nonconvex-Nonconcave Minimax Optimization: Primal-Dual Balancing and Iteration Complexity Analysis [https://arxiv.org/abs/2209.10825 \[arXiv\]]\n
Jiajin Li, *Linglingzhi Zhu*, Anthony Man-Cho So. \n
preprint, 2023.
. LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees [https://lzzhuling.github.io/files/LogSpecT-NeurIPS23.pdf \[pdf\]]\n
Shangyuan Liu, *Linglingzhi Zhu*, Anthony Man-Cho So. \n 
Advances in Neural Information Processing Systems 36 (NeurIPS 2023).
. Riemannian Linearized Proximal Algorithms for Nonnegative Inverse Eigenvalue Problem [https://lzzhuling.github.io/files/RLPA-NA23.pdf \[pdf\]] \n 
(α-β) Sangho Kum, Chong Li, Jinhua Wang, Jen-Chih Yao, *Linglingzhi Zhu*. \n
Numerical Algorithms (2023) 94:1819-1848. 
. Linearized Proximal Algorithms with Adaptive Stepsizes for Convex Composite Optimization with Applications [https://lzzhuling.github.io/files/ALPA-AMO23.pdf \[pdf\]][https://lzzhuling.github.io/files/slides_ALPA.pdf \[slides\]] \n 
(α-β) Yaohua Hu, Chong Li, Jinhua Wang, Xiaoqi Yang, *Linglingzhi Zhu*. \n
Applied Mathematics & Optimization (2023) 87:52. 
. Nonsmooth Composite Nonconvex-Concave Minimax Optimization [https://lzzhuling.github.io/files/NNCC-OPT22.pdf \[pdf\]][https://lzzhuling.github.io/files/slides_NNC_C.pdf \[slides\]]\n
Jiajin Li, *Linglingzhi Zhu*, Anthony Man-Cho So. \n 
NeurIPS 2022 Workshop on Optimization for Machine Learning (OPT 2022), Oral. 
. Orthogonal Group Synchronization with Incomplete Measurements: Error Bounds and Linear Convergence of the Generalized Power Method [https://arxiv.org/abs/2112.06556 \[arXiv\]][https://lzzhuling.github.io/files/slides_sync_EB.pdf \[slides\]] \n
*Linglingzhi Zhu*, Jinxin Wang, Anthony Man-Cho So. \n
preprint, 2021.

== Professional Service
Reviewer for
- SIAM Journal on Optimization
- Mathematics of Operations Research
- IEEE Transactions on Information Theory
- Information and Inference: A Journal of the IMA
- International Conference on Machine Learning (ICML) 2023, 2024
- Neural Information Processing Systems (NeurIPS) 2022, 2023
- International Conference on Learning Representations (ICLR) 2024
- NeurIPS Workshop on Optimization for Machine Learning  (OPT) 2023
